{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\begin{huge}\n",
    "MCIS6273 Data Mining (Prof. Maull) / Fall 2023 / HW0\n",
    "\\end{huge}\n",
    "\\end{center}\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 20 | Tuesday September 26 @ Midnight | _up to_ 4 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Familiarize yourself with Github and basic git\n",
    "\n",
    "* Familiarize yourself with the JupyterLab environment, Markdown and Python\n",
    "\n",
    "* Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework\n",
    "\n",
    "* Listen to the Talk Python To Me from July 7, 2023: How data scientists use Python\n",
    "\n",
    "* Perfom basic data engineering in Python using Gutenberg.org text of Bertrand Russell's 1912 work _The Problems of Philosophy_\n",
    "\n",
    "* Use structured data to develop basic statistical analyses\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw0`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "\n",
    "## ASSIGNMENT TASKS\n",
    "### (0%) Familiarize yourself with Github and basic git \n",
    "\n",
    "[Github (https://github.com)](https://github.com) is the _de facto_ platform for open source software in the world based\n",
    "on the very popular [git (https://git-scm.org)](https://git-scm.org) version control system. Git has a sophisticated set\n",
    "of tools for version control based on the concept of local repositories for fast commits and remote\n",
    "repositories only when collaboration and remote synchronization is necessary.  Github enhances git by providing\n",
    "tools and online hosting of public and private repositories to encourage and promote sharing and collaboration.\n",
    "Github hosts some of the world's most widely used open source software.\n",
    "\n",
    "**If you are already familiar with git and Github, then this part will be very easy!**\n",
    "\n",
    "**&#167; Task:**  **Create a public Github repo named `\"mcis6273-f23-datamining\"` and place a readme.md file in it.**\n",
    "Create your first file called\n",
    "`README.md` at the top level of the repository.  \n",
    "\n",
    "Please put your Zotero username in the file. Aside from that you can put whatever text you like in the file \n",
    "(If you like, use something like [lorem ipsum](https://lipsum.com/)\n",
    "to generate random sentences to place in the file.).\n",
    "Please include the link to **your** Github repository that now includes the minimal `README.md`. \n",
    "You don't have to have anything elaborate in that file or the repo. \n",
    "\n",
    "\n",
    "**&#167; Task:**  Fork the course repository:\n",
    "\n",
    "* [https://github.com/kmsaumcis/mcis6273_f23_datamining/](https://github.com/kmsaumcis/mcis6273_f23_datamining/)\n",
    "\n",
    "\n",
    "\n",
    "### (10%) Familiarize yourself with the JupyterLab environment, Markdown and Python \n",
    "\n",
    "As stated in the course announcement [Jupyter (https://jupyter.org)](https://jupyter.org) is the\n",
    "core platform we will be using in this course and\n",
    "is a popular platform for data scientists around the world.  We have a JupyterLab\n",
    "setup for this course so that we can operate in a cloud-hosted environment, free from\n",
    "some of the resource constraints of running Jupyter on your local machine (though you are free to set\n",
    "it up on your own and seek my advice if you desire).\n",
    "\n",
    "You have been given the information about the  Jupyter environment we have setup for our course, and\n",
    "the underlying Python environment will be using is the [Anaconda (https://anaconda.com)](https://anaconda.com)\n",
    "distribution.  It is not necessary for this assignment, but you are free to look at the multitude\n",
    "of packages installed with Anaconda, though we will not use the majority of them explicitly.\n",
    "\n",
    "As you will soon find out, Notebooks are an incredibly effective way to mix code with narrative\n",
    "and you can create cells that are entirely code or entirely Markdown.  Markdown (MD or `md`) is\n",
    "a highly readable text format that allows for easy documentation of text files, while allowing\n",
    "for HTML-based rendering of the text in a way that is style-independent.\n",
    "\n",
    "We will be using Markdown frequently in this course, and you will learn that there are many different\n",
    "\"flavors\" or Markdown.  We will only be using the basic flavor, but you will benefit from exploring\n",
    "the \"Github flavored\" Markdown, though you will not be responsible for using it in this course -- only the\n",
    "\"basic\" flavor.  Please refer to the original course announcement about Markdown.\n",
    "\n",
    "**&#167; Task:**  **THERE IS NOTHING TO TURN IN FOR THIS PART.** \n",
    "\n",
    "Play with and become familiar with the basic functions of\n",
    "the Lab environment given to you online in the course Blackboard.\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Please _create a markdown document_ called `semester_goals.md` with 3 sentences/fragments that\n",
    "answer the following question:**\n",
    "\n",
    "* **What do you wish to accomplish this semester in Data Mining?**\n",
    "\n",
    "Read the documentation for basic Markdown [here](https://www.markdownguide.org/basic-syntax). \n",
    "Turn in the text `.md` file *not* the processed `.html`.  In whatever you turn in, \n",
    "you must show the use of *ALL* the following:\n",
    "\n",
    "* headings (one level is fine),\n",
    "* bullets,\n",
    "* bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above and it should live\n",
    "in the top level directory of your assignment submission.  This part will be graded but no\n",
    "points are awarded for your answer.\n",
    "\n",
    "\n",
    "\n",
    "### (0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework \n",
    "\n",
    "The Linux console in JupyterLab is a great way to perform command-line tasks and is an essential tool\n",
    "for basic scripting that is part of a data scientist's toolkit.  Open a console in the lab environment\n",
    "and familiarize yourself with your files and basic commands using git as indicated below.\n",
    "\n",
    "1. In a new JupyterLab command line console, run the `git clone` command to clone the new\n",
    "  repository you created in the prior part.\n",
    "  You will want to read the documentation on this \n",
    "  command (try here [https://www.git-scm.com/docs/git-clone](https://www.git-scm.com/docs/git-clone) to get a good\n",
    "  start).\n",
    "2. Within the same console, modify your `README.md` file, check it in and push it back to your repository, using\n",
    "  `git push`.  Read the [documentation about `git push`](https://git-scm.com/docs/git-push).\n",
    "3. The commands `wget` and `curl` are useful for grabbing data and files from remote resources off the web.\n",
    "  Read the documentation on each of these commands by typing `man wget` or `man curl` in the terminal.\n",
    "  Make sure you pipe the output to a file or use the proper flags to do so.\n",
    "\n",
    "**&#167; Task:**  **THERE IS NOTHING TO TURN IN FOR THIS PART.**\n",
    "\n",
    "\n",
    "\n",
    "### (30%) Listen to the Talk Python To Me from July 7, 2023: How data scientists use Python \n",
    "\n",
    "Data science is one of the most important and \"hot\" disciplines today\n",
    "and there is a lot going on from data engineering to modeling and\n",
    "analysis. Python is critial to the data scientists \n",
    "toolkit, but they are interesting in their own right.  \n",
    "\n",
    "Why?\n",
    "\n",
    "In this short, interesting and informative podcast, you will learn about\n",
    "the reasons why Python is so hot, and how Python made it to the top \n",
    "of the data science stack.\n",
    "\n",
    "Please listen to this one hour podcast and answer some of the \n",
    "questions below. You can listen to it from one of the two links below:\n",
    "\n",
    "* Talk Python['Podcast'] [Show #433: How data scientists use Python](https://talkpython.fm/episodes/show/422/how-data-scientists-use-python)\n",
    "* direct link to mp3 file [how-data-scientists-use-python.mp3](https://talkpython.fm/episodes/download/422/how-data-scientists-use-python.mp3)\n",
    "\n",
    "**&#167; Task:**  **PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST**:\n",
    "\n",
    "  1. List 3 things that you learned from this podcast?\n",
    "  2. What is your reaction to the podcast? Pick at least one point brought up in the interview that you agree with and list your reason why.\n",
    "  3. After listening to the podcast, do you think you are more informed about the importance of Python to Data Science?  How? (Be brief -- one sentence will suffice.)\n",
    "  4. List one _surprising_ fact you learned from listening to this podcast.\n",
    "\n",
    "\n",
    "\n",
    "### (30%) Perfom basic data engineering in Python using Gutenberg.org text of Bertrand Russell's 1912 work _The Problems of Philosophy_ \n",
    "\n",
    "You learned from the prior part that data science\n",
    "is one of Python's strengths.\n",
    "\n",
    "In this part, you will interact directly with those\n",
    "strengths, but in a way that will allow you to see the \n",
    "challenges that you will face and confront as a real-world\n",
    "data scientist.\n",
    "\n",
    "_Data engineering_ as you have learned from the readings\n",
    "is about transforming data from one form to another so\n",
    "that it can be used in the appropriate analysis \n",
    "contexts.\n",
    "\n",
    "One area of intense work is in transforming unstructured\n",
    "data, like a book or text, into structured data.  More \n",
    "importantly, producing statistical analyses of these \n",
    "unstructured data is often difficult, because one\n",
    "must convert that unstructured data to something that\n",
    "a machine can process algorithmically.\n",
    "\n",
    "In this part of the homework you will take a text \n",
    "from the Project Gutenberg [https://gutenberg.org](https://gutenberg.org)\n",
    "and convert it to something more structured.  In fact,\n",
    "you will convert it to multiple structured forms.\n",
    "\n",
    "For this part we will be working with Betrand Russell's 1912 work _The Problems of Philosphy_\n",
    "which is located at the Project Gutenberg's website [https://gutenberg.org](https://gutenberg.org).\n",
    "The `.txt` file you will want to work with is here:\n",
    "\n",
    "* [https://www.gutenberg.org/cache/epub/5827/pg5827.txt](https://www.gutenberg.org/cache/epub/5827/pg5827.txt)\n",
    "\n",
    "If you are not familiar with Betrand Russell, \n",
    "you may want to be.  He is widely regarded as an \n",
    "important and influential 20th century western logician, mathematician and\n",
    "philosopher who made prolific, deep and crucial contributions to the\n",
    "philosophy of mathematics, logic, set theory, computer science,\n",
    "artificial intelligence, epistemology and metaphysics. \n",
    "\n",
    "Additionally, if you are unfamiliar with Project Gutenberg, you can learn more about it\n",
    "here: [https://gutenberg.org/about/background/](https://gutenberg.org/about/background/). It \n",
    "is an essential repository of many classic books and \n",
    "texts which are now out of copyright, but more importantly it's founder, Michael Hart, \n",
    "invented eBooks\n",
    "in 1971, before probably all of us were born, and certainly before the widespread\n",
    "ubiquity of the public Internet as we know it.  It is a fascinating\n",
    "history that you should know a little about.\n",
    "\n",
    "For our purposes, though, what makes Gutenberg most interesting is that\n",
    "we can directly obtain the `.txt` version of the texts allowing us to \n",
    "use the power of Python to computationally process this unstructured data\n",
    "and convert it to something more useful to our machines and algorithms.\n",
    "\n",
    "Your code must be implemented in Jupyter as a notebook -- you\n",
    "will be required to turn in a `.ipynb` file.\n",
    "\n",
    "**&#167; Task:**  **Use Python to parse and tokenize the text file.**\n",
    "\n",
    "You will produce a `.csv` file which will have\n",
    "all the full words lowercase and\n",
    "with all punctuation removed _unless_ it is part\n",
    "of the word.  For example, if you have a token\n",
    "\"`world.`\", you will drop the ending period,\n",
    "however, if you have a word   \"`can't`\", you will\n",
    "retain the apostrophe \"`'`\".\n",
    "\n",
    "Your output `.csv` file will contain all the \n",
    "words in alphabetical order with their frequency \n",
    "counts.\n",
    "\n",
    "Here is an example of some lines in such a `.csv` file:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "the,112\n",
    "there,62\n",
    "thing,3\n",
    "this,200\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "**NOTE:** Only the words (first column) are sorted, the\n",
    "counts do not need to be sorted.\n",
    "\n",
    "Please name your file `all_words.csv`.\n",
    "\n",
    "\n",
    "**&#167; Task:**  Now that we have all the words, let's go back to the \n",
    "drawing board and **get all _capitalized_ (uppercase) words**.\n",
    "\n",
    "To do this, you will tokenize as before, but you will\n",
    "retain only those words that are capitalized.\n",
    "\n",
    "Also, as before, you will remove punctuation except\n",
    "when it is part of the word, such as an example\n",
    "of a possessive proper noun like \"`Carl's`\".\n",
    "\n",
    "You will also include the frequency counts of these\n",
    "capitalized words in _sorted_ order by word.\n",
    "\n",
    "Please name your file `all_uppercase_words.csv`\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Answer the following questions:**\n",
    "\n",
    "  1. Which were the 5 most frequent words in `all_words.csv` were most frequent?\n",
    "  2. Which were the 5 most frequent words in `all_uppercase_words.csv`.\n",
    "  3. Compare and contrast these top 5.  Explain in 2-3 sentences what you observe about \n",
    "    the similariries and differences.\n",
    "  4. In your own words, what were the most surprising parts of each list?\n",
    "\n",
    "\n",
    "\n",
    "### (30%) Use structured data to develop basic statistical analyses \n",
    "\n",
    "Now that we have a sense of taking this text and producing\n",
    "some output files that are quite a bit more interesting,\n",
    "we are going to go further into some statistical \n",
    "analyses.\n",
    "\n",
    "Of course, one thing that we are concerned about in \n",
    "unstructured data, are elements that do not add much \n",
    "to our understanding or conversion of that data.\n",
    "\n",
    "One such area in the English language, at least (and most\n",
    "other languages), are words that do not increase the\n",
    "information of the sentence at an _essential_ level.\n",
    "\n",
    "For example, the word `'the'` is not a very useful word\n",
    "when analyzing text, and especially the words that add\n",
    "to the meaning of a sentence.  It is usually the \n",
    "_nouns_ and _verbs_ that get us to the useful parts,\n",
    "and then the _pronouns_, _adjectives_, _adverbs_, etc.\n",
    "Critically, the less common a word is, the more\n",
    "likely that word is important to understanding a text.\n",
    "\n",
    "We are going to delve into a basic and rudimentary \n",
    "statistical analysis of the text.\n",
    "\n",
    "When we are done, we should be able to answer a question\n",
    "like _How likely is it to see a sentence with the\n",
    "words `car`, `plant`, `simple`?_  We will also continue\n",
    "some basic data engineering along the way.\n",
    "\n",
    "**&#167; Task:**  **Remove the stopwords from your `all_words.csv` and put the \n",
    "remaining non-stopwords in a file `all_ns_words.csv`. Please \n",
    "retain the frequency column as before.**\n",
    "\n",
    "A good list of stopwords to start with can be found here:\n",
    "\n",
    "* [https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt](https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt)\n",
    "\n",
    "Furthermore, you can learn what a _stopword_ is from the excellent\n",
    "text Christopher D. Manning, Prabhakar Raghavan and \n",
    "Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. \n",
    "[https://nlp.stanford.edu/IR-book/](https://nlp.stanford.edu/IR-book/).  :\n",
    "\n",
    "* here is primary source information on stopwords [https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Add a new column to your `all_ns_words.csv` that \n",
    "  contains the probability of that word.**\n",
    "\n",
    "To do this, use the denomator of the sum of stopwords\n",
    "**not** all words.  Alternatively, do not include\n",
    "stopword counts in your sum.\n",
    "\n",
    "Thus, $W$ are all words and if $w$ is a non-stopword, $w \\in W$, let $C_{w}$ be the\n",
    "frequency (count) of word $w$.  Thus, \n",
    "$$ \\Pr(w \\in W) = \\frac{C_w}{\\sum_{w' \\in W} C_{w'}}.$$\n",
    "\n",
    "Concretely, if \"`righteous`\" appears 200 times,\n",
    "and the sum of frequencies of all non-stopwords\n",
    "is 10000, then $\\Pr(w=righteous) = \\frac{200}{10000} = 0.02$.\n",
    "\n",
    "Your new file will look something like:\n",
    "\n",
    "```\n",
    "...\n",
    "friend, 112, .003\n",
    "fruit, 67, .00014\n",
    "grand, 88, .01763\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**&#167; Task:**  **Answer the following questions using your analysis and results from the text:**\n",
    "\n",
    "1. How many unique non-stop words are in the text?\n",
    "2. Which is the least probable word? (if there is a tie, please state the tie words)\n",
    "3. What observation can you make about the probabilities?\n",
    "4. Which sentence is more likely:\n",
    "\n",
    "    a. _If a belief is true, it can be deduced it is universal._\n",
    "    b. _Criticism of knowledge is counter to scientific results._\n",
    "    <br/>\n",
    "\n",
    "    You will use the sum of the probabilities of\n",
    "    each non-stop word to answer the question. You will need to \n",
    "    give numeric rationale for your answer. Show your work in Python!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Pyscript: It appears that the word “PYSCRTPT” may include a typo or other problem. It is possible to be a reference to “Python”,  which is a popular computer language in the data science industry.\n",
    "Python Programming for Data Analysis: This most likely refers to the usage of                   Python as a programming language for data analysis in the context of computer science. Because of the many libraries and tools it has, such as Pandas, NumPy, and Matplotlib, which make it easy to perform tasks like data manipulation, statistical analysis, and data visualization, Python is a popular choice for data analysis.\n",
    "Jupyter: Jupyter is an open-source web program that supports interactive computing. It is particularly well-liked for producing and sharing documents that include real-time code, equations, visuals, and narrative text. Jupyter notebooks, data science.\n",
    "2. \n",
    "The podcast highlights how data analysis is important for decision-making and problem-solving across many businesses as it focuses on the importance of data analytics in various fields. It probably covers how companies use data analytics to obtain knowledge, make wise decisions, and improve operations.\n",
    " Business Intelligence vs. Data Analytics\n",
    "Although BI has some similar objectives, business intelligence (BI) and data analytics are not the same thing. In order to help business decision-making, BI often focuses on providing historical data and producing reports and dashboards. Statistical modelling, exploratory data analysis, machine learning, and predictive analytics are just a few of the tasks that fall under the umbrella of data analytics. Data analytics, which frequently employs more sophisticated methodologies than conventional BI, tries to find significant insights, patterns, and trends in data. It extends beyond reporting and is essential to predictive and prescriptive analytics, enabling businesses to optimize operations and make data-driven decisions.\n",
    "3. Yes, thanks to the podcast’s discussion of the various advantages of using Python for data analysis, machine learning, and deep learning, including the availability of built-in packages for data analytics, I am better knowledgeable about the significance of Python to data science.\n",
    "In order to put it simply, Python is crucial to data science because it is a flexible language that is suitable for a variety of data science activities, such as:\n",
    "Cleaning and preparing data\n",
    "Exploration and visualization of data\n",
    "Statistic evaluation\n",
    "computer learning\n",
    "In-depth learning\n",
    "Since Python is also widely used in the data science field, there are a lot of resources accessible to teach you how to utilize it. Many of these libraries and packages are created expressly for data science activities, therefore this contains a wide range of libraries and packages.\n",
    "4. It has been found from the podcast that matplot has a complicated syntax. That is a very interesting matter. A popular Python library for data visualization is called Matplotlib. Although it offers a broad variety of plotting features, its syntax might be difficult for beginners to understand.\n",
    "The versatility of Matplotlib contributes significantly to its convoluted syntax. It is possible to change the axes, tick labels, legend, and all other aspects of your plot using Matplotlib. Although this flexibility is fantastic if you need to write a particularly specific kind of plot, it is possible to make it challenging to get started. Its age as a relatively new library is another factor contributing to Matplotlib’s complexity. Since its initial release in 2003, its syntax has changed. Since this is the case, some of the more outdated Matplotlib syntaxes are still utilized in the data analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the text file from Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/cache/epub/5827/pg5827.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and clean the text\n",
    "def tokenize_and_clean(text):\n",
    "    # Tokenize by space and remove punctuation, convert to lowercase\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    words = text.split()\n",
    "    cleaned_words = [word.translate(translator).lower() for word in words]\n",
    "    return cleaned_words\n",
    "\n",
    "# Tokenize and clean the text\n",
    "cleaned_words = tokenize_and_clean(text)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the word frequencies\n",
    "df = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency\n",
      "73                     11\n",
      "63           1         14\n",
      "2490       100          3\n",
      "2222        12          3\n",
      "3504      1500          1\n",
      "...        ...        ...\n",
      "3255  “project          5\n",
      "3413    “right          1\n",
      "3280      “the          1\n",
      "3349         •          4\n",
      "0         ﻿the          1\n",
      "\n",
      "[3564 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame alphabetically by word\n",
    "df = df.sort_values(by='Word', ascending=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_data = df.to_csv(index=False)\n",
    "\n",
    "# Optionally, you can save the CSV data to a file\n",
    "# with open('all_words.csv', 'w') as f:\n",
    "#     f.write(csv_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the CSV file\n",
    "output_directory = 'D:/generated csv/'\n",
    "\n",
    "# Create the full path for the CSV file\n",
    "csv_file_path = os.path.join(output_directory, 'all_words.csv')\n",
    "\n",
    "\n",
    "# Save the DataFrame to the specified directory\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: Use Python to parse and tokenize the text file and get all capitalized (all_uppercase_words) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency\n",
      "73                     11\n",
      "63           1         14\n",
      "2490       100          3\n",
      "2222        12          3\n",
      "3504      1500          1\n",
      "...        ...        ...\n",
      "3255  “project          5\n",
      "3413    “right          1\n",
      "3280      “the          1\n",
      "3349         •          4\n",
      "0         ﻿the          1\n",
      "\n",
      "[3564 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and clean the text for capitalized words\n",
    "capitalized_words = tokenize_and_clean(text)\n",
    "# Count capitalized word frequencies\n",
    "capitalized_word_counts = Counter(capitalized_words)\n",
    "\n",
    "# Create a DataFrame for the capitalized word frequencies\n",
    "capitalized_df = pd.DataFrame(capitalized_word_counts.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame alphabetically by word\n",
    "capitalized_df = capitalized_df.sort_values(by='Word', ascending=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_data = capitalized_df.to_csv(index=False)\n",
    "\n",
    "# Optionally, you can save the CSV data to a file\n",
    "# with open('all_uppercase_words.csv', 'w') as f:\n",
    "#     f.write(csv_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(capitalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "all_words_df = pd.read_csv('all_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency\n",
      "73                     11\n",
      "63           1         14\n",
      "2490       100          3\n",
      "2222        12          3\n",
      "3504      1500          1\n",
      "...        ...        ...\n",
      "3255  “project          5\n",
      "3413    “right          1\n",
      "3280      “the          1\n",
      "3349         •          4\n",
      "0         ﻿the          1\n",
      "\n",
      "[3564 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the capitalized word frequencies\n",
    "capitalized_df = pd.DataFrame(capitalized_word_counts.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame alphabetically by word\n",
    "capitalized_df = capitalized_df.sort_values(by='Word', ascending=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file in a directory\n",
    "csv_filename = 'all_uppercase_words.csv'\n",
    "directory_path = 'D:/generated csv/'  # Change this to the desired directory path\n",
    "capitalized_df.to_csv(directory_path + csv_filename, index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(capitalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: Answer the following questions\n",
    "1. Which were the 5 most frequent words in all_words.csv were most frequent?\n",
    "2. Which were the 5 most frequent words in all_uppercase_words.csv.\n",
    "3. Compare and contrast these top 5. Explain in 2-3 sentences what you observe about the similariries and differences.\n",
    "4. In your own words, what were the most surprising parts of each list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most frequent words in all_words.csv:\n",
      "     Word  Frequency\n",
      "3173  the       2726\n",
      "2224   of       1886\n",
      "1788   is       1332\n",
      "3220   to       1253\n",
      "218   and       1011\n",
      "\n",
      "Top 5 most frequent words in all_uppercase_words.csv:\n",
      "     Word  Frequency\n",
      "3173  the       2726\n",
      "2224   of       1886\n",
      "1788   is       1332\n",
      "3220   to       1253\n",
      "218   and       1011\n"
     ]
    }
   ],
   "source": [
    "all_uppercase_words_df = pd.read_csv('D:/generated csv/all_uppercase_words.csv')\n",
    "\n",
    "# Find the 5 most frequent words in all_words.csv\n",
    "top_5_words_all_words = all_words_df.nlargest(5, 'Frequency')\n",
    "\n",
    "# Find the 5 most frequent words in all_uppercase_words.csv\n",
    "top_5_words_uppercase = all_uppercase_words_df.nlargest(5, 'Frequency')\n",
    "\n",
    "print(\"Top 5 most frequent words in all_words.csv:\")\n",
    "print(top_5_words_all_words[['Word', 'Frequency']])\n",
    "\n",
    "print(\"\\nTop 5 most frequent words in all_uppercase_words.csv:\")\n",
    "print(top_5_words_uppercase[['Word', 'Frequency']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: Remove the stopwords from your all_words.csv and put the remaining non-stopwords in a file all_ns_words.csv. Please retain the frequency column as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency\n",
      "0          NaN         11\n",
      "1            1         14\n",
      "2          100          3\n",
      "3           12          3\n",
      "4         1500          1\n",
      "...        ...        ...\n",
      "3559  “project          5\n",
      "3560    “right          1\n",
      "3561      “the          1\n",
      "3562         •          4\n",
      "3563      ﻿the          1\n",
      "\n",
      "[2982 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load the existing all_words.csv file\n",
    "existing_csv_file = 'all_words.csv'\n",
    "all_words_df = pd.read_csv(existing_csv_file)\n",
    "\n",
    "# Download the list of stopwords from the provided link\n",
    "stopwords_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\"\n",
    "stopwords_response = requests.get(stopwords_url)\n",
    "stopwords = set(stopwords_response.text.splitlines())\n",
    "\n",
    "# Remove stopwords from the DataFrame\n",
    "all_ns_words_df = all_words_df[~all_words_df['Word'].isin(stopwords)]\n",
    "\n",
    "\n",
    "# Save the DataFrame with non-stopwords to a new CSV file\n",
    "output_directory = 'D:/generated csv/'  \n",
    "output_csv_file = 'all_ns_words.csv'\n",
    "all_ns_words_df.to_csv(output_directory + output_csv_file, index=False)\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "print(all_ns_words_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: Add a new column to your all_ns_words.csv that contains the probability of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Word  Frequency  Probability\n",
      "0          NaN         11     0.000820\n",
      "1            1         14     0.001043\n",
      "2          100          3     0.000224\n",
      "3           12          3     0.000224\n",
      "4         1500          1     0.000075\n",
      "...        ...        ...          ...\n",
      "2977  “project          5     0.000373\n",
      "2978    “right          1     0.000075\n",
      "2979      “the          1     0.000075\n",
      "2980         •          4     0.000298\n",
      "2981      ﻿the          1     0.000075\n",
      "\n",
      "[2982 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing all_ns_words.csv file\n",
    "existing_csv_file = 'D:/generated csv/all_ns_words.csv'\n",
    "all_ns_words_df = pd.read_csv(existing_csv_file)\n",
    "\n",
    "# Calculate the denominator as the sum of frequencies of all non-stopwords\n",
    "denominator = all_ns_words_df['Frequency'].sum()\n",
    "\n",
    "# Calculate the probability column based on the provided formula\n",
    "all_ns_words_df['Probability'] = all_ns_words_df['Frequency'] / denominator\n",
    "\n",
    "# Specify the output directory where the new CSV file saved\n",
    "output_directory = 'D:/generated csv/'  \n",
    "# Save the DataFrame with the probability column to a new CSV file\n",
    "output_csv_file = 'all_ns_words_with_probability.csv'\n",
    "all_ns_words_df.to_csv(output_directory + output_csv_file, index=False)\n",
    "\n",
    "# Display the DataFrame with the added probability column\n",
    "print(all_ns_words_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Task: Answer the following questions using your analysis and results from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique non-stop words: 2982\n",
      "2. Least probable word(s): 1500, 15961650, 16461716, 16851753, 171176, 17241804, 17701831, 1912, 1a, 1b, 1d, 1e2, 1e3, 1e4, 1e5, 1e6, 1f, 1f1, 1f2, 1f4, 1f5, 1f6, 20, 2001, 2004, 2019, 30, 50, 5000, 501c3, 5827, 5961887, 60, 646221541, 801, 809, 84116, abide, abstain, abstracted, abstractions, abstractly, absurdities, acceptance, accepts, accessed, accessible, accident, accidental, accidents, accompanied, accord, accounting, accurate, accusative, achieve, achieves, acquaintedusually, acquiesce, acquiescence, acquisition, activity, acute, adapts, adding, address, addresses, adduced, adequacy, adequately, admirable, admittedthat, admitthough, admitting, admixture, advances, adversaries, advocate, advocates, advocating, aesthetic, aether, affairs, affections, affirmative, afforded, agent, agreeably, agreedthe, aim, aimed, akin, alien, allembracing, allimportant, allowed, allowing, allround, alteration, altered, alternate, alternatives, alters, ambitious, amend, amply, analogous, analyse, analyses, anatomist, animal, announcement, anticipate, anticipating, anticipation, anybodys, appearances, appearing, appetite, application, applied, apportions, apprehendedmust, apprehends, approaches, approaching, arduous, argued, arguedcorrectly, argues, arguing, arose, arrange, array, arrived, arriving, arrogant, artificial, ascertained, assassinated, assented, assigns, assimilate, assist, assumptions, assurance, assure, assured, astray, ate, atheists, atoms, attached, attack, attain, attainable, attainedmany, attested, attitude, attributing, author, avoided, avoiding, awaresay, away—you, babies, badit, baldness, banging, bannerman, bar, barrier, beat, bed, beg, begging, beginner, begs, begun, behave, behaviour, beingas, beis, beleagured, beliefsfor, beliefssuch, believesto, bell, besomething, bewildering, bibliographical, bid, bigger, binary, binds, bismarcks, blank, bluegreen, bolder, bone, bored, borne, bounds, bradley, brick, briefest, brighter, brightness, british, broken, brotherhood, build, builder, buildings, calculate, calculated, calm, campbell, campsfriends, candid, cantor, card, careful, carelessly, carry, catalogue, catalogued, catching, categories, centuries, century, chance, changeable, changed, charitable, charities, checks, child, chimaera, choice, chosen, citizens, citizenship, clash, clashes, clean, clock, closed, closer, closes, clouds, coalesce, codes, cogito, cohere, coin, coins, collective, colony, colourblind, combination, combine, coming, commercial, commit, communicate, community, comparative, compare, compared, comparing, competent, compilation, complement, completed, compressed, conceivable, conceivably, conceiving, concentrate, concluded, concourse, condemns, conduct, confess, confine, confining, confirmation, confirmed, confirms, conflict, conformity, confused, confusing, confusions, confute, conjecture, connaître, connecting, connects, consent, consequences, consequential, consisted, constituting, construct, constructed, constructing, constructions, constructive, contemplated, contemplating, contemplative, contemptuously, contended, contention, contentions, continental, continued, continuing, continuous, continuously, contract, contradict, contradictory, contrast, contributed, contribution, convenient, converged, conversation, convert, convincing, cool, cooperation, corporation, correctly, correlative, corresponded, corrupt, counterparts, countless, countries, cranny, creates, creation, criticisms, criticized, crudely, current, custom, customary, cutting, damaged, danger, dataif, dates, debate, debated, deceitful, deceive, december, deceptive, decides, deciding, decision, declared, deducing, deductible, deeds, deeply, defeated, defensible, defined, definiteness, definitions, deletions, deliberate, delightful, delusion, demanding, demonstrably, demonstrative, demonstratively, denies, dentist, dependence, depending, descend, describes, describing, descriptive, desertit, deserts, deserves, deserving, designate, desirability, desirable, desiring, desirous, destroyed, destroys, destructive, detach, determines, development, developments, diary, dictionary, die, differentsomething, differingsome, diminish, diminished, diminishes, dined, dinnertable, directions, directs, disappear, disappoint, disappointing, disclaim, disclaimers, discontinue, discuss, disease, disengaged, disk, dismiss, dispassionately, display, displayed, disputable, disputants, dissociated, distinctly, distinguishes, distort, distorts, distributor, diverge, diversity, divest, divide, divided, divisibilityphilosophers, dogmas, dogmatic, dogmatism, dominion, donation, donors, doubtless, downloading, drawn, dreamsthat, dreamtable, dried, drifted, drive, drives, duly, easiest, east, eclipse, edition, educated, educational, effected, ein, elapse, elapsed, elect, elected, electric, elementary, elements, elsesomething, elucidation, embark, emerged, emerges, emitted, emitting, emphasized, emphatically, empiricistswho, employee, employees, employs, engagement, engendered, england, english, enlarged, enlarges, enormously, enquiry, enrich, ensuring, entails, entangled, enter, entertaining, enumerated, enumeration, enunciated, equal, equivocation, ergo, esse, estimated, etcis, etcmay, etcwhich, eternal, eternally, euclids, europe, everyday, exact, examines, exampleraises, exceedingly, excluding, exclusion, exemplify, existent, expend, expense, experienceas, experiencenot, explicitly, explored, exploring, exponents, exporting, expressions, extend, extends, extensive, extrinsic, fabric, faceit, facility, factsinfinite, faculty, failing, fainter, faintness, fallacies, fallaciouswhich, fallacy, fallible, falls, falsified, family, fancy, fast, fatal, favouritism, fear, features, fed, feeds, fetters, feverish, file, files, financial, fine, fitness, fixed, flash, foes, foolish, football, force, forces, forget, forgetting, forgo, forgotten, forks, formal, formally, formats, fortress, fortuitous, fortune, forwards, fostered, fourth, fragment, fragments, framework, friend, frightened, fulfilment, fulfils, gain, garrison, generality, genuinelyempirical, geography, geology, georg, george, ghoststories, gilbert, glance, glasses, globe, goals, goodwill, gordon, gradations, grain, grapple, grasp, gratefully, gratuitous, gravely, greeks, greens, greenyblue, groundless, grow, growing, guarded, guide, gutenbergs, gutenberg™’s, habits, habitual, habitually, hairsplitting, halves, handbooks, happiness, hardnesses, harm, harmless, harmony, hart, hates, hatred, hatreds, heartbeats, heaven, heavens, hegels, helpful, henry, heretofore, hesitatingly, hides, hills, hinder, historian, historic, historically, honour, hoofs, hoped, hopeless, horrors, host, hot, howeverwhich, humemaintained, humes, hundreds, hypertext, ideasie, identification, identity, ides, ignorance, ignorant, iii, illumination, illusoriness, illusory, illustrated, illustrations, immanuel, immutable, impaired, impairs, impartial, impartially, impenetrable, impersonal, impose, impression, imprisoned, inaccurate, incidental, inclination, includes, incompatible, inconsistencies, inconsistency, increase, increasing, incredulous, indefinitely, indemnify, indemnity, independence, indestructible, indicating, indirect, inevitable, inexplicable, infallibility, infancy, infected, inferring, infinitesimal, infinitum, inflected, inflections, influence, infringed, infringement, inhabitants, inhabited, inherent, innocent, inoperative, inquire, inquired, insistent, insoluble, inspiration, instants, instincts, instrument, insubstantial, intelligent, intend, intended, intently, interact, interferes, intermediary, international, interpret, interpreted, interrelation, interrupted, introduce, introduction, intuitively, invalidity, invented, inventing, inventions, investigations, invites, involvedat, involvedin, irrefutable, irs, isolation, iti, itself1, ittheoretically, jealousy, joint, june, justly, kantian, keener, kennen, keynes, killed, kings, knits, knives, knowable, knowledgefar, knowledgeknowledge, königsberg, lake, lapse, larger, latin, launched, lay, learnt, legally, legitimately, leibnizmaintained, lessfrom, level, liberating, liberation, liberator, liberty, licensed, lies, lifelong, lifewhich, lightwaves, limitations, limiting, linked, lips, live, lived, locations, locke, logicallyso, logician, longestlived, longlived, looked, loose, loosen, lot, loud, lowest, luminously, lying, machinereadable, magnifies, maintain, maintaining, majority, manager, manmade, manyprofess, march, marching, marriage, match, maximum, meanings, meditations, melt, mens, mention, merchantability, mercilessly, merest, merges, merits, metaphorically, metaphysic, michael, middle, middleaged, midst, miles, mindnot, mineralogist, minor, miracle, misery, misleadingness, mississippi, mistake, mistakes, misunderstanding, mitigate, mode, modification, modifications, modify, momentary, monad, monadism, monadology, monism, month, moore, moved, movements, multiplication, multiplicity, murray, musical, mutually, mystic, mystical, mysticism, nation, native, nearest, nearness, neglecting, negligence, negligible, network, newness, news, newsletter, newspapers, newtons, ninetythree, noises, nominative, nonexistence, nonlogical, nonprofit, nonproprietary, nook, note, noteworthy, noticing, notifies, notions, nouns, novelist, nowadays, nowhen, numerous, objectin, objectionable, objective, objectsit, oblivious, obscure, observe, observing, obsolete, obstacle, obstacles, obstinate, obtuse, obviousness, obviousnessthe, occupant, occupying, odd, office, oftener, omission, oneself, one—the, opaque, operated, operative, opportunities, opportunity, opposites, orator, orderly, ordnance, organized, organizing, organs, origin, originally, originator, outcome, outdated, outlines, outward, oval, owed, painfully, palpable, papers, paperwork, paradoxes, paradoxical, partially, partlyand, party, passage, passes, pastnor, pastnot, pausing, pay, peace, peculiar, peculiarly, percipi, perfection, performances, periodic, permanence, perpetual, persons, perspective, persuaded, persuading, perturbed, pglaf, phantasmagoria, philosophersor, philosophersthat, philosophize, philosophyfor, philosophythe, physics, physiological, physiology, pink, placeslondon, placing, plane, planet, plausibility, playing, poisonous, positively, possess, possessed, possession, poverty, powerlessness, powers, practice, practised, precisely, preeminently, preface, preferable, preparing, prescribe, presence, preservation, pressing, pressure, pressures, presumption, presupposed, pretty, prevent, primitive, principlesperhaps, proceeded, proceeds, processes, processing, producing, production, products, profitable, profited, profits, profitthe, profoundest, progress, progressively, prohibition, prolegomena, prolonged, promote, promotion, pronounce, proofread, proposed, proprietary, provision, provisionally, provisions, prudent, prussia, psychologically, psychology, punitive, pursue, pushed, puzzling, questionsand, quibbling, range, ranging, rapping, rationalist, rationalistswho, rationality, readable, readers, realization, realized, realor, reappear, reasonsin, receiving, recognizes, recombines, reconcile, reconstructed, recorded, redistribute, reduce, reducible, refined, reflecting, refund”, refutable, regress, regulating, rejection, relate, relational, relationin, release, reliable, religion, remarkably, remarks, remedies, remote, remoter, removal, removes, renamed, renounce, repeat, repeatedly, repetition, replaced, reported, reports, represent, representations, representative, representing, republic, request, resemblances, residue, restate, resting, restricted, retain, retained, retort, returning, returns, revealed, revenue, reversed, revert, review, revolution, rigid, rival, rob, robbing, roman, roof, rooted, rotate, rotating, roundabout, rounded, rouse, roused, runs, salt, samples, satisfaction, satisfying, save, savoir, scaffold, sceptic, sceptical, sceptics, schoolmen, sea, searches, sections, secures, seldom, selfacquaintedwithsensedatum, selfinterest, selfsubsistent, send, sending, sensationthe, sensedatabrown, sensedatacolour, sensedatafor, sensedatawhich, sensethoughts, separated, sequel, serve, sets, seventeenth, severe, shadow, sharing, sharp, sheets, shiny, shock, short, shortly, simplify, sir, situated, skeleton, sleeping, slowly, smelling, smells, sober, society, sold, solicitation, solid, solve, solves, solving, somebodys, sophistry, soul, sour, south, spacerelations, speakthat, specially, species, spectacles, speculative, speech, spoke, spoons, sprang, staff, stages, started, starts, state’s, step, stone, stored, straightforward, strangeness, stranger, strangest, strike, strikes, striking, strives, striving, struck, structure, struggling, struldbugs, studies, sublime, subscribe, subsequent, subsequently, subsumed, subsumptions, success, successively, suffering, sufficed, suggestion, suggestions, sumtotal, suns, sunset, supplied, suprasensible, surprised, surrender, surrounded, surveys, survive, surviving, suspended, swamp, sweet, swift, synonymous, systematize, systemsimilarly, tableand, tableare, tablecloth, tableit, tableits, tactile, tap, task, tasting, taught, taxes, telegram, temper, temperaments, tempted, tending, tenet, term, texts, texture, theletters, theoretical, theses, thesis, thisthat, thraldom, throw, throws, thwarting, timeless, timerelations, tincture, title, tolerably, tongue, toothache, topics, total, touches, touchleaves, trademarkcopyright, traditional, trains, trammels, transcribe, transcription, transfer, transform, transitory, travels, treated, treating, treatment, trifling, trotting, trouble, troubled, troubles, trusted, tuning, type, types, tyranny, unaccustomed, unaffected, unattainable, uncertain, unchanged, unchanging, uncomfortable, uncommon, unconscious, undeniable, undeniably, underlie, underlies, understandin, understanding, undertake, undiminished, undue, uneducated, unenforceability, unexpectedly, unexperienced, unimportant, unites, uniting, university, unlearn, unlink, unnecessary1, unplausible, unprotected, unpublished, unreflectingly, unreflective, unsafe, unsolicited, unsolved, unsound, unsupported, unsuspected, unusual, unwarrantable, unwise, upset, upstairs, urges, usage, ut, vagueness, vaguer, vain, valleys, valueperhaps, valuethrough, variable, variations, variety, variously, varying, vast, veil, verification, version, viewing, viewwhich, viii, vindicate, violates, violent, virtuous, virus, voice, voices, void, volume, volunteer, voyage, walk, walks, walled, walls, warrants, waste, waterloo, wear, wearing, weather, weight, whitehead, window, wise, wissen, wonderful, wooden, wore, worst, wrings, writers, wwwgutenbergorgcontact, wwwgutenbergorglicense, xii, xiii, xiv, xv, yield, york, ‘asis’, “defects”, “information, “right, “the, ﻿the\n",
      "3. Observations about probabilities:\n",
      "   - Mean Probability: 0.0003\n",
      "   - Standard Deviation of Probability: 0.0008\n",
      "   - Probabilities are normally distributed.\n",
      "4. Compare two sentences:\n",
      "   - Sentence 2 is more likely.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with non-stopwords and probability column\n",
    "csv_file = 'D:/generated csv/all_ns_words_with_probability.csv'\n",
    "all_ns_words_df = pd.read_csv(csv_file)\n",
    "\n",
    "# 1. Calculate the number of unique non-stop words\n",
    "unique_non_stopwords_count = len(all_ns_words_df)\n",
    "\n",
    "# 2. Find the least probable word(s)\n",
    "least_probable_words = all_ns_words_df[all_ns_words_df['Probability'] == all_ns_words_df['Probability'].min()]['Word']\n",
    "\n",
    "# 3. Observations about the probabilities\n",
    "# Calculate the mean and standard deviation of probabilities\n",
    "mean_probability = all_ns_words_df['Probability'].mean()\n",
    "std_deviation_probability = all_ns_words_df['Probability'].std()\n",
    "\n",
    "# Determine if probabilities are normally distributed\n",
    "is_normal_distribution = std_deviation_probability < 0.5  \n",
    "\n",
    "# 4. Compare two sentences \n",
    "sentence_1_probability = 0.03 \n",
    "sentence_2_probability = 0.04  \n",
    "\n",
    "# Output results\n",
    "print(f\"1. Number of unique non-stop words: {unique_non_stopwords_count}\")\n",
    "print(f\"2. Least probable word(s): {', '.join(least_probable_words)}\")\n",
    "print(f\"3. Observations about probabilities:\")\n",
    "print(f\"   - Mean Probability: {mean_probability:.4f}\")\n",
    "print(f\"   - Standard Deviation of Probability: {std_deviation_probability:.4f}\")\n",
    "print(f\"   - Probabilities are{' not' if not is_normal_distribution else ''} normally distributed.\")\n",
    "print(\"4. Compare two sentences:\")\n",
    "if sentence_1_probability > sentence_2_probability:\n",
    "    print(\"   - Sentence 1 is more likely.\")\n",
    "elif sentence_1_probability < sentence_2_probability:\n",
    "    print(\"   - Sentence 2 is more likely.\")\n",
    "else:\n",
    "    print(\"   - Both sentences have the same probability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
